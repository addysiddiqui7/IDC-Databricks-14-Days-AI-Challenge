{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c05d34c-ce25-4834-a254-dd3973ee8721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DAY 13 (21/01/26) – Model Comparison & Feature Engineering\n",
    "### Learn:\n",
    "\n",
    "- Training multiple models\n",
    "- Hyperparameter tuning\n",
    "- Feature importance\n",
    "- Spark ML Pipelines\n",
    "\n",
    "### \uD83D\uDEE0️ Tasks:\n",
    "\n",
    "1. Train 3 different models\n",
    "2. Compare metrics in MLflow\n",
    "3. Build Spark ML pipeline\n",
    "4. Select best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "955ab667-55a1-4be5-bb66-c25d464ab656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#TASK 1: Train 3 Different Models (sklearn)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a18099b7-c67b-4b74-bb81-b9ec6f0a245c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "df = spark.table(\"ecommerce_catalog.default.events_gold\").toPandas()\n",
    "\n",
    "df = df.fillna({\n",
    "    \"views\": 0,\n",
    "    \"carts\": 0,\n",
    "    \"revenue\": 0,\n",
    "    \"purchases\": 0\n",
    "})\n",
    "\n",
    "X = df[[\"views\", \"carts\", \"revenue\"]]\n",
    "y = df[\"purchases\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c125ee42-73b2-4d17-bfdf-c52501b5e344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/01/21 16:09:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_regression → R² = 0.9807\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/01/21 16:09:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision_tree → R² = 0.9671\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[31m2026/01/21 16:09:28 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_forest → R² = 0.9811\n"
     ]
    }
   ],
   "source": [
    "#Training multiple models & logging to MLflow..\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "models = {\n",
    "    \"linear_regression\": LinearRegression(),\n",
    "    \"decision_tree\": DecisionTreeRegressor(max_depth=5, random_state=42),\n",
    "    \"random_forest\": RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    with mlflow.start_run(run_name=name):\n",
    "\n",
    "        mlflow.log_param(\"model_type\", name)\n",
    "        mlflow.log_param(\"features\", \",\".join(X.columns))\n",
    "        mlflow.log_param(\"test_size\", 0.2)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        mlflow.log_metric(\"r2_score\", r2)\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        print(f\"{name} → R² = {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36568f45-28aa-44fc-9bdd-a32d5c798081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### TASK 3: Spark ML Pipeline ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c17eb6a-1b26-4148-84ae-4b742eef3be6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark_df = spark.table(\"ecommerce_catalog.default.events_gold\") \\\n",
    "    .fillna(0, subset=[\"views\", \"carts\", \"revenue\", \"purchases\"])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"views\", \"carts\", \"revenue\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"purchases\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5e3497f-a42c-46f5-82fc-39589fdf8cc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Pipeline R²: 0.9956\n"
     ]
    }
   ],
   "source": [
    "#Train & evaluate..\n",
    "\n",
    "train_df, test_df = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "predictions = pipeline_model.transform(test_df)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"purchases\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(f\"Spark Pipeline R²: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c0c35c-ebef-4c6f-826d-039e6ab2516a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------------+---------+--------------------+\n|views|carts|           revenue|purchases|          prediction|\n+-----+-----+------------------+---------+--------------------+\n|    1|    0|               0.0|        0|-0.17804818047412965|\n| 4612|   62|           10175.5|       23|   12.13981321741502|\n| 5291|   87|19951.839999999997|       40|   22.89343925460553|\n|  307|    1|               0.0|        0| -1.1574762388707485|\n|    5|    0|               0.0|        0|-0.19851958764584596|\n+-----+-----+------------------+---------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\n",
    "    \"views\", \"carts\", \"revenue\", \"purchases\", \"prediction\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8417e49a-d76f-4e1d-bbca-ba9ac7e84bb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### TASK 4: Select Best Model (Final Decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c598d622-6d44-4c58-9d6d-4633e1bbb620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Selected Spark ML Pipeline with Linear Regression as the final model due to:\n",
    "\n",
    "Highest R² score\n",
    "\n",
    "Scalability on large datasets\n",
    "\n",
    "Clean production-ready pipeline design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71d42c54-4a0e-4389-92e2-fd41e837f07f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "day 13",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}