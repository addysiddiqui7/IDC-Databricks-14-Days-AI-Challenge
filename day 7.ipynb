{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79e7d230-36df-4a85-9bb1-8bd62e50c91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **DAY 7 (15/01/26) – Workflows & Job Orchestration**\n",
    "\n",
    "### Learn:\n",
    "\n",
    "- Databricks Jobs vs notebooks\n",
    "- Multi-task workflows\n",
    "- Parameters & scheduling\n",
    "- Error handling\n",
    "\n",
    "### \uD83D\uDEE0️ Tasks:\n",
    "\n",
    "1. Add parameter widgets to notebooks\n",
    "2. Create multi-task job (Bronze→Silver→Gold)\n",
    "3. Set up dependencies\n",
    "4. Schedule execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f0024ea-eb03-46cb-8c0c-0a98a1ec76ea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Task parameter"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running task: bronze\n"
     ]
    }
   ],
   "source": [
    "dbutils.widgets.text(\"task\", \"bronze\", \"Pipeline Task\")\n",
    "task = dbutils.widgets.get(\"task\")\n",
    "print(\"Running task:\", task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03a582f0-527f-4420-bcc4-204da6552387",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bronze"
    }
   },
   "outputs": [],
   "source": [
    "if task == \"bronze\":\n",
    "    df = spark.read.csv(\n",
    "        \"/Volumes/workspace/ecommerce/ecommerce_data/2019-Nov.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "\n",
    "    from pyspark.sql import functions as F\n",
    "\n",
    "    events_bronze = df.withColumn(\"ingestion_ts\", F.current_timestamp())\n",
    "\n",
    "    events_bronze.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"events_bronze\")\n",
    "\n",
    "    print(\"Bronze stage completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24437440-8628-4d9a-ae20-65e9bb4d5052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if task == \"silver\":\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.window import Window\n",
    "\n",
    "    bronze = spark.table(\"events_bronze\")\n",
    "\n",
    "    w = Window.partitionBy(\"user_session\", \"event_time\", \"product_id\").orderBy(F.col(\"ingestion_ts\").desc())\n",
    "\n",
    "    silver = (\n",
    "        bronze\n",
    "        .filter(F.col(\"event_time\").isNotNull())\n",
    "        .filter(F.col(\"product_id\").isNotNull())\n",
    "        .filter(F.col(\"user_id\").isNotNull())\n",
    "        .filter(F.col(\"price\").isNotNull())\n",
    "        .filter((F.col(\"price\") >= 0) & (F.col(\"price\") < 10000))\n",
    "        .filter(F.col(\"event_type\").isin(\"view\", \"cart\", \"purchase\"))\n",
    "        .withColumn(\"rn\", F.row_number().over(w))\n",
    "        .filter(F.col(\"rn\") == 1)\n",
    "        .drop(\"rn\")\n",
    "        .withColumn(\"event_date\", F.to_date(\"event_time\"))\n",
    "        .withColumn(\n",
    "            \"price_tier\",\n",
    "            F.when(F.col(\"price\") < 10, \"budget\")\n",
    "             .when(F.col(\"price\") < 50, \"mid\")\n",
    "             .otherwise(\"premium\")\n",
    "        )\n",
    "        .drop(\"ingestion_ts\")\n",
    "    )\n",
    "\n",
    "    silver.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"events_silver\")\n",
    "\n",
    "    print(\"Silver stage completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "862f4d46-1c04-43cc-8598-bff29f4ed747",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Gold"
    }
   },
   "outputs": [],
   "source": [
    "if task == \"gold\":\n",
    "    from pyspark.sql import functions as F\n",
    "\n",
    "    silver = spark.table(\"events_silver\")\n",
    "\n",
    "    gold = (\n",
    "        silver.groupBy(\"product_id\", \"brand\", \"category_code\")\n",
    "        .agg(\n",
    "            F.countDistinct(F.when(F.col(\"event_type\") == \"view\", F.col(\"user_id\"))).alias(\"views\"),\n",
    "            F.countDistinct(F.when(F.col(\"event_type\") == \"cart\", F.col(\"user_id\"))).alias(\"carts\"),\n",
    "            F.countDistinct(F.when(F.col(\"event_type\") == \"purchase\", F.col(\"user_id\"))).alias(\"purchases\"),\n",
    "            F.sum(F.when(F.col(\"event_type\") == \"purchase\", F.col(\"price\"))).alias(\"revenue\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"conversion_rate_pct\",\n",
    "            F.round(F.col(\"purchases\") / F.col(\"views\") * 100, 2)\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"avg_order_value\",\n",
    "            F.round(F.col(\"revenue\") / F.col(\"purchases\"), 2)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    gold.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"events_gold_products\")\n",
    "\n",
    "    print(\"Gold stage completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d002968e-cdae-408c-9b30-d9fb4ed10dd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Databricks Pipeline Execution Logic\n",
    "\n",
    "This notebook is designed to run as part of a Databricks **multi-task job**.\n",
    "\n",
    "The behavior of the notebook is controlled by the `task` parameter:\n",
    "\n",
    "- `task = bronze`  \n",
    "  Reads new raw CSV files and writes them into the Delta Bronze table (`events_bronze`) with an ingestion timestamp.\n",
    "\n",
    "- `task = silver`  \n",
    "  Reads from `events_bronze`, applies data quality rules, deduplication, and enrichment, and writes the cleaned data into `events_silver`.\n",
    "\n",
    "- `task = gold`  \n",
    "  Reads from `events_silver` and computes business aggregates, writing results into `events_gold_products`.\n",
    "\n",
    "The Databricks Job runs this same notebook three times per execution:\n",
    "Bronze → Silver → Gold, ensuring correct dependency order and automated daily processing.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "day 7",
   "widgets": {
    "task": {
     "currentValue": "bronze",
     "nuid": "f51a7012-89b9-4bbd-ace9-d58ee4d9ebee",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bronze",
      "label": "Pipeline Task",
      "name": "task",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "bronze",
      "label": "Pipeline Task",
      "name": "task",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}